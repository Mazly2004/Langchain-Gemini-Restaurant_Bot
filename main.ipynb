{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1c94b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c3e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash-preview-04-17\",\n",
    "    temperature = 0.6,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b6c8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common and respectful way to say \"I love you mom\" in Shona is:\n",
      "\n",
      "**Ndinokuda, Amai.**\n",
      "\n",
      "You could also use:\n",
      "\n",
      "*   **Ndinokuda, Mhamha.** (Slightly more informal/affectionate term for mom)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant that translates English to Shona.\"),\n",
    "    (\"human\", \"I love you mom.\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ad26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating chains in langchain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\" , \"You are a helpful assistant that suggests a single fancy restaurant name.\"),\n",
    "    (\"human\" , \"I want to open a restaurant for {cuisine} food.\")\n",
    "])\n",
    "\n",
    "chain_1 = prompt | llm\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\" , \"You are a helpful assistant that suggests menu items for a given restaurant.\"),\n",
    "    (\"human\" , \"Give me items for {restaurant} restaurant as comma seperated list.\")\n",
    "])\n",
    "\n",
    "chain_2 = prompt_2 | llm\n",
    "\n",
    "overall_chain = chain_1 | chain_2\n",
    "\n",
    "response2 = overall_chain.invoke({\n",
    "    \"cuisine\" : \"Mexican\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe1d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obsidian Shard Crisps, Smoked Mushroom Caps, Volcanic Ash Steak, Squid Ink Pasta, Ember Roasted Root Vegetables, Dark Chocolate Lava Cake, Black Sesame Ice Cream\n"
     ]
    }
   ],
   "source": [
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c18a4a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RunnableParallel' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     21\u001b[39m step_1 = RunnablePassthrough.assign(\n\u001b[32m     22\u001b[39m     restaurant = chain_1   \n\u001b[32m     23\u001b[39m )\n\u001b[32m     25\u001b[39m step_2 = RunnableParallel(\n\u001b[32m     26\u001b[39m     restaurant_name = (\u001b[38;5;28;01mlambda\u001b[39;00m x : x[\u001b[33m\"\u001b[39m\u001b[33mrestaurant\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     27\u001b[39m     menu_items = chain_2 ,\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m overall_chain = step_1 | \u001b[43mstep_2\u001b[49m\u001b[43m  \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmenu_items\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m | output_parser\n\u001b[32m     33\u001b[39m response2 = overall_chain.invoke({\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcuisine\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mMexican\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m })\n",
      "\u001b[31mTypeError\u001b[39m: 'RunnableParallel' object is not callable"
     ]
    }
   ],
   "source": [
    "#To create a sequential chain \n",
    "from langchain_core.runnables import RunnablePassthrough , RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\" , \"You are a helpful assistant that suggests a single fancy restaurant name.\"),\n",
    "    (\"human\" , \"I want to open a restaurant for {cuisine} food.\")\n",
    "])\n",
    "\n",
    "chain_1 = prompt | llm | output_parser\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\" , \"You are a helpful assistant that suggests menu items for a given restaurant.\"),\n",
    "    (\"human\" , \"Give me items for {restaurant} restaurant as comma seperated list.\")\n",
    "])\n",
    "\n",
    "chain_2 = prompt_2 | llm | output_parser\n",
    "\n",
    "step_1 = RunnablePassthrough.assign(\n",
    "    restaurant = chain_1   \n",
    ")\n",
    "\n",
    "step_2 = RunnableParallel(\n",
    "    restaurant_name = (lambda x : x[\"restaurant\"]),\n",
    "    menu_items = chain_2 ,\n",
    "\n",
    ")\n",
    "\n",
    "overall_chain = step_1 | step_2  (lambda x : x[\"menu_items\"]) | output_parser\n",
    "\n",
    "response2 = overall_chain.invoke({\n",
    "    \"cuisine\" : \"Mexican\",\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65322766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant_name': '**Oro de MÃ©xico**', 'menu_items': 'Guacamole, Nachos, Quesadillas, Sopa de Tortilla, Taco Salad, Carne Asada Tacos, Al Pastor Tacos, Fish Tacos, Enchiladas Rojas, Enchiladas Verdes, Carne Asada Burrito, Carnitas Burrito, Chicken Fajitas, Steak Fajitas, Chile Relleno, Carnitas Plate, Carne Asada Plate, Camarones a la Diabla, Molcajete, Combination Plate, Flan, Churros, Margaritas, Horchata'}\n"
     ]
    }
   ],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3792d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
